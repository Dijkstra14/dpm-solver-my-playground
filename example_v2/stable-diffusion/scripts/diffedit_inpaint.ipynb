{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUmmV5ZvrPbP"
   },
   "source": [
    "# DiffEdit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92QkRfm0e6K0"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "import random\n",
    "from PIL import Image\n",
    "from einops import repeat\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.dpm_solver import DPMSolverSampler\n",
    "from ldm.models.diffusion.dpm_solver import (\n",
    "    model_wrapper,\n",
    "    NoiseScheduleVP,\n",
    "    DPM_Solver\n",
    ")\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_img(path, opt):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    #     w, h = image.size\n",
    "    #     print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    #     w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((opt.W, opt.H), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2. * image - 1.\n",
    "\n",
    "\n",
    "def latent_to_image(model, latents):\n",
    "    x_samples = model.decode_first_stage(latents)\n",
    "    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "    x_samples = x_samples.cpu().permute(0, 2, 3, 1).numpy()\n",
    "    x_samples = 255. * x_samples\n",
    "    x_samples = x_samples.astype(np.uint8)\n",
    "\n",
    "    return x_samples\n",
    "\n",
    "\n",
    "def repeat_tensor(x, n, dim=0):\n",
    "    dims = len(x.shape) * [1]\n",
    "    dims[dim] = n\n",
    "    return x.repeat(dims)\n",
    "\n",
    "\n",
    "def get_mask(model, src, dst, init_latent, n: int, ddim_steps,\n",
    "             clamp_rate:float=3):\n",
    "    \"\"\"\n",
    "    the map value will be clamped to map.mean() * clamp_rate, then values will be scaled into 0~1, then term into binary(split at 0.5).\n",
    "    so if a map value is large than map.mean() * clamp_rate * 0.5 will be encode to 1, less will be encode to 0. \n",
    "    so the larger clamp rate is, less pixes will be encode to 1, the small clamp rate is, the more pixes will be encode to 1.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    repeated = repeat_tensor(init_latent, n)\n",
    "    src = repeat_tensor(src, n)\n",
    "    dst = repeat_tensor(dst, n)\n",
    "\n",
    "\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=model.num_timesteps,\n",
    "                              trained_betas=model.betas.cpu().numpy())\n",
    "    scheduler.set_timesteps(ddim_steps, device=device)\n",
    "    noised = scheduler.add_noise(repeated, noise,\n",
    "                                 scheduler.timesteps[ddim_steps // 2]\n",
    "                                 )\n",
    "\n",
    "    t = scheduler.timesteps[ddim_steps // 2]\n",
    "    t_ = torch.unsqueeze(t, dim=0).to(device)\n",
    "    pre_src = model.apply_model(noised, t_, src)\n",
    "    pre_dst = model.apply_model(noised, t_, dst)\n",
    "\n",
    "    # consider to add smooth method\n",
    "    subed = (pre_src - pre_dst).abs_().mean(dim=[0, 1])\n",
    "    max_v = subed.mean() * clamp_rate\n",
    "    mask = subed.clamp(0, max_v) / max_v\n",
    "\n",
    "    def to_binary(pix):\n",
    "        if pix > 0.5:\n",
    "            return 1.\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "    mask = mask.cpu().apply_(to_binary).to(device)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def diffedit(model, init_image,\n",
    "             src_prompt: str = \"A bowl of fruits\",\n",
    "             dst_prompt: str = \"A bowl of pears\",\n",
    "             encode_ratio: float = 0.6,\n",
    "             ddim_steps: int = 20,\n",
    "             seed: int = 42,\n",
    "             scale: float = 7.5,\n",
    "             precision=\"autocast\"):\n",
    "    \"\"\"\n",
    "    :param init_image: image to be edit\n",
    "    :param src_prompt: prompt describe origin image(i.e. A bowl of fruits)\n",
    "    :param dst_prompt: prompt describe desired image(i.e. A bowl of pears)\n",
    "    :param encode_ratio: how deep to encode origin image, must between 0-1\n",
    "    :param ddim_steps: total ddim steps, actual encode steps = ddim_steps * encode ratio\n",
    "    :param seed: random seed\n",
    "    :param scale: classifier free guidance scale\n",
    "    :param precision: ema precision\n",
    "    \"\"\"\n",
    "    #If seed is None, randomly select seed from 0 to 2^32-1\n",
    "    if seed is None:\n",
    "        seed = random.randrange(2 ** 32 - 1)\n",
    "    seed_everything(seed)\n",
    "    device = model.device\n",
    "\n",
    "    model.cond_stage_model = model.cond_stage_model.to(device)\n",
    "    precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "    assert os.path.isfile(opt.origin_image)\n",
    "    init_image = load_img(opt.origin_image, opt).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(device.type):\n",
    "            with model.ema_scope():\n",
    "                uc = None\n",
    "                if scale != 1.0:\n",
    "                    uc = model.get_learned_conditioning([\"\"])\n",
    "                src = model.get_learned_conditioning([src_prompt])\n",
    "                dst = model.get_learned_conditioning([dst_prompt])\n",
    "                init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))\n",
    "\n",
    "                # get mask\n",
    "                mask = get_mask(model, src, dst, init_latent, 3, ddim_steps)\n",
    "                plt.imshow(mask.detach().cpu().numpy())\n",
    "                plt.show()\n",
    "                \n",
    "                ns = NoiseScheduleVP('discrete', betas=model.betas)\n",
    "                model_fn = model_wrapper(\n",
    "                    lambda x, t, c: model.apply_model(x, t, c),\n",
    "                    ns,\n",
    "                    model_type=\"noise\",\n",
    "                    guidance_type=\"classifier-free\",\n",
    "                    condition=src,\n",
    "                    unconditional_condition=uc,\n",
    "                    guidance_scale=scale\n",
    "                )\n",
    "\n",
    "                # add noise and record each step's output latent\n",
    "                noiser = DPM_Solver(model_fn, ns, predict_x0=True, thresholding=False)\n",
    "\n",
    "                noised_sample, record_list = noiser.sample(\n",
    "                    init_latent,\n",
    "                    t_start=1. / model.num_timesteps,\n",
    "                    t_end=encode_ratio,\n",
    "                    method='multistep',\n",
    "                    order=2,\n",
    "                    steps=ddim_steps,\n",
    "                    return_intermediate=True\n",
    "                )\n",
    "\n",
    "                # perform step wise edit\n",
    "                model_fn_dst = model_wrapper(\n",
    "                    lambda x, t, c: model.apply_model(x, t, c),\n",
    "                    ns,\n",
    "                    model_type=\"noise\",\n",
    "                    guidance_type=\"classifier-free\",\n",
    "                    condition=dst,\n",
    "                    unconditional_condition=uc,\n",
    "                    guidance_scale=scale\n",
    "                )\n",
    "                solver = DPM_Solver(model_fn_dst, ns, predict_x0=True, thresholding=False)\n",
    "\n",
    "                solver.sample_edit = sample_edit.__get__(solver, type(solver))\n",
    "                recover = solver.sample_edit(\n",
    "                    noised_sample,\n",
    "                    t_start=encode_ratio,\n",
    "                    t_end=1. / model.num_timesteps,\n",
    "                    method='multistep',\n",
    "                    order=2,\n",
    "                    steps=ddim_steps,\n",
    "                    mask=mask,\n",
    "                    record_list=list(reversed(record_list))\n",
    "                )\n",
    "\n",
    "                images = latent_to_image(model, recover)\n",
    "                return images\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--prompt\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=\"A bowl of fruits\",\n",
    "        help=\"the prompt to render\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--edit\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=\"A bowl of pears\",\n",
    "        help=\"the edit prompt\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--ddim_steps\",\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help=\"number of ddim sampling steps\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--H\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"image height, in pixel space\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--W\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"image width, in pixel space\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--scale\",\n",
    "        type=float,\n",
    "        default=7.5,\n",
    "        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        default=\"configs/stable-diffusion/v1-inference.yaml\",\n",
    "        help=\"path to config which constructs model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ckpt\",\n",
    "        type=str,\n",
    "        default=\"models/ldm/stable-diffusion-v1/model.ckpt\",\n",
    "        help=\"path to checkpoint of model\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=42,\n",
    "        help=\"the seed (for reproducible sampling)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--precision\",\n",
    "        type=str,\n",
    "        help=\"evaluate at this precision\",\n",
    "        choices=[\"full\", \"autocast\"],\n",
    "        default=\"autocast\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--init_img\",\n",
    "        type=str,\n",
    "        help=\"the path of image to be edit\",\n",
    "        default=None,\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--outdir\",\n",
    "        type=str,\n",
    "        help=\"the path of image to be edit\",\n",
    "        default=None,\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    # opt = parser.parse_args()\n",
    "    opt = parser.parse_args(args=[\n",
    "        \"--scale\", \"7.5\",\n",
    "        \"--ddim_steps\", \"20\",\n",
    "        \"--seed\", \"42\",\n",
    "        \"--ckpt\", \"/mfs/xiangchendong19/stable-diffusion-ckpt/sd-v1-4-full-ema.ckpt\",\n",
    "        \"--prompt\", \"A bowl of fruits\",\n",
    "        \"--edit\", \"A bowl of pears\" ,\n",
    "        \"--outdir\", \"output/\",\n",
    "        \"--init_img\", \"data/fruit.png\"\n",
    "    ])\n",
    "\n",
    "    config = OmegaConf.load(f\"{opt.config}\")\n",
    "    model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(opt.init_img)\n",
    "init_image = load_img(opt.init_img, opt).to(device)\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=1)\n",
    "print(opt.prompt)\n",
    "print(opt.edit)\n",
    "res = diffedit(model, init_image,\n",
    "                src_prompt=opt.prompt,\n",
    "                dst_prompt=opt.edit,\n",
    "                ddim_steps=opt.ddim_steps,\n",
    "                seed=opt.seed,\n",
    "                scale=opt.scale,\n",
    "                precision=opt.precision\n",
    "                )\n",
    "if not os.path.exists(opt.outdir):\n",
    "    os.mkdir(opt.outdir)\n",
    "Image.fromarray(res[0]).save(opt.outdir + \"/diffedit.png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "latent-imagenet-diffusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
